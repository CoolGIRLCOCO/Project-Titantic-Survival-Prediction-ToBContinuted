{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Survival on Titanic Datasets Using Classification Methods(Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Apply Machine Learning Algorithms\n",
    "In module 1, I started with exploratory data analysis (EDA) in order to understand the problem and find the hidden information inside each feature. After that I reengineer the existing features to modify them and create new features to better explain the dataset to machine learning model. \n",
    "\n",
    "In this module, our goal is to identify the relationship between survived or not(target variable) with other features. Please see the table of content as follows:\n",
    "\n",
    "### Table of Content\n",
    "* 3.0 Import Packages\n",
    "* 3.1 Read Dataset\n",
    "* 3.2 Dummy Variables Encoding\n",
    "* 3.3 Feature Normalization(Feature Scaling)\n",
    "* 3.4 Split Dataset\n",
    "* 3.5 Introduction of Evaluation Matrics\n",
    "* 3.6 Machine Learning Methods\n",
    "* 3.7 Model Comparison\n",
    "* 3.8 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.0 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import get_dummies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Read Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Age</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Title</th>\n",
       "      <th>NumFamily</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>S</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>C</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>S</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>Miss</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35.0</td>\n",
       "      <td>S</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35.0</td>\n",
       "      <td>S</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>Mr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId   Age Embarked     Fare  Pclass     Sex  Survived Title  \\\n",
       "0            1  22.0        S   7.2500       3    male         0    Mr   \n",
       "1            2  38.0        C  71.2833       1  female         1   Mrs   \n",
       "2            3  26.0        S   7.9250       3  female         1  Miss   \n",
       "3            4  35.0        S  53.1000       1  female         1   Mrs   \n",
       "4            5  35.0        S   8.0500       3    male         0    Mr   \n",
       "\n",
       "   NumFamily  \n",
       "0          1  \n",
       "1          1  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data = pd.read_csv(\"Titanic_Data_Preparation.csv\") \n",
    "TestIndex = pd.read_csv(\"TestDataIndex.csv\",header = None,names=['0','Index'])\n",
    "Data.shape#1309 x 9\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dummy Variables Encoding\n",
    "As we can see above, there are several categorical variables which are stored as text values, including column 'Sex' (male, female), 'Embarked' (S, C, Q) and 'Title' (Mr, Mrs, Miss, Master, Others). Regardless of what the value is used for, the challenge is determining how to use data in the analysis. As we know, many machine learning algorithms can deal with categorical variables without any manipulation but there are also some algorithms cannot do. How to turn these text attributes into numarics for further processing? Read http://pbpython.com/categorical-encoding.html if you want to know more methods about dealing with categorical variables.\n",
    "\n",
    "At the beginning, we may think of converting categorical variables into ordinal numbers, however, categorical variables cannot tell us which levels should be more important than others, but ordinal numbers can. I think this method will make algorithms bias. \n",
    "\n",
    "In order to unify our data and easy to do comparison based on the ML results, we will create dummy variables to convert each category value into a new column and assigns a 1 or 0 (True/False) value to the column. This has the benefit of not weighting a value improperly but does have the downside of adding more columns to the data set. To avoid multicolinearity (Dummy Variable Trap), I plan to remove the first dummy variables of all categorical variables, such as 'Sex_female', 'Title_Master' and 'Embarked_C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "      <th>NumFamily</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Others</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId   Age     Fare  Survived  NumFamily  Sex_female  Sex_male  \\\n",
       "0            1  22.0   7.2500         0          1           0         1   \n",
       "1            2  38.0  71.2833         1          1           1         0   \n",
       "2            3  26.0   7.9250         1          0           1         0   \n",
       "3            4  35.0  53.1000         1          1           1         0   \n",
       "4            5  35.0   8.0500         0          0           0         1   \n",
       "\n",
       "   Title_Master  Title_Miss  Title_Mr  Title_Mrs  Title_Others  Embarked_C  \\\n",
       "0             0           0         1          0             0           0   \n",
       "1             0           0         0          1             0           1   \n",
       "2             0           1         0          0             0           0   \n",
       "3             0           0         0          1             0           0   \n",
       "4             0           0         1          0             0           0   \n",
       "\n",
       "   Embarked_Q  Embarked_S  Pclass_1  Pclass_2  Pclass_3  \n",
       "0           0           1         0         0         1  \n",
       "1           0           0         1         0         0  \n",
       "2           0           1         0         0         1  \n",
       "3           0           1         1         0         0  \n",
       "4           0           1         0         0         1  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "      <th>NumFamily</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Others</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId   Age     Fare  Survived  NumFamily  Sex_male  Title_Miss  \\\n",
       "0            1  22.0   7.2500         0          1         1           0   \n",
       "1            2  38.0  71.2833         1          1         0           0   \n",
       "2            3  26.0   7.9250         1          0         0           1   \n",
       "3            4  35.0  53.1000         1          1         0           0   \n",
       "4            5  35.0   8.0500         0          0         1           0   \n",
       "\n",
       "   Title_Mr  Title_Mrs  Title_Others  Embarked_Q  Embarked_S  Pclass_2  \\\n",
       "0         1          0             0           0           1         0   \n",
       "1         0          1             0           0           0         0   \n",
       "2         0          0             0           0           1         0   \n",
       "3         0          1             0           0           1         0   \n",
       "4         1          0             0           0           1         0   \n",
       "\n",
       "   Pclass_3  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  \n",
       "3         0  \n",
       "4         1  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data0 = pd.get_dummies(Data,columns = ['Sex','Title','Embarked','Pclass'])\n",
    "Data1 = Data0.drop(['Sex_female', 'Title_Master', 'Embarked_C','Pclass_1'],axis = 1)\n",
    "Data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Normalization(Feature Scaling)\n",
    "We also notice there are some features are in different scales, such as column 'Age' and 'Fare'. These two features are numerical variables. When we apply ML methods like KNN, SVM, Neural Networks, etc. different scales of input features may give different contributions to the ML algorithms and then cause problems. So transform the input features into same scale is necessary. We may consider to use mean-std normalization \n",
    "<img src = \"normalized.png\" width = 280>\n",
    "or min-max normalization\n",
    "<img src = \"minmax.png\" width = 340>\n",
    "as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "      <th>NumFamily</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Others</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.273456</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.473882</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.323563</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.436302</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.436302</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId       Age      Fare  Survived  NumFamily  Sex_male  Title_Miss  \\\n",
       "0            1  0.273456  0.014151         0          1         1           0   \n",
       "1            2  0.473882  0.139136         1          1         0           0   \n",
       "2            3  0.323563  0.015469         1          0         0           1   \n",
       "3            4  0.436302  0.103644         1          1         0           0   \n",
       "4            5  0.436302  0.015713         0          0         1           0   \n",
       "\n",
       "   Title_Mr  Title_Mrs  Title_Others  Embarked_Q  Embarked_S  Pclass_2  \\\n",
       "0         1          0             0           0           1         0   \n",
       "1         0          1             0           0           0         0   \n",
       "2         0          0             0           0           1         0   \n",
       "3         0          1             0           0           1         0   \n",
       "4         1          0             0           0           1         0   \n",
       "\n",
       "   Pclass_3  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  \n",
       "3         0  \n",
       "4         1  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data2 = Data1.copy()\n",
    "#Data2.Age=(Data2.Age-Data2.Age.mean())/Data2.Age.std()\n",
    "#Data2.Fare=(Data2.Fare-Data2.Fare.mean())/Data2.Fare.std()\n",
    "Data2.Age=(Data2.Age-Data2.Age.min())/(Data2.Age.max()-Data2.Age.min())\n",
    "Data2.Fare=(Data2.Fare-Data2.Fare.min())/(Data2.Fare.max()-Data2.Fare.min())\n",
    "\n",
    "Data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Split Dataset\n",
    "\n",
    "In module 1, we did explanatory data analysis on whole dataset. Since the dataset is ready now, we will split the dataset, train models use a number of machine learning algorithms on train set and do evaluation & model comparison to find the optimal algorithm based on evaluation metric scores on test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of trainset is (891, 14)\n",
      "Dimension of testset is:  (418, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['PassengerId', 'Age', 'Fare', 'Survived', 'NumFamily', 'Sex_male',\n",
       "       'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Others', 'Embarked_Q',\n",
       "       'Embarked_S', 'Pclass_2', 'Pclass_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Testlist = TestIndex.Index.tolist()\n",
    "Train = Data2.loc[~Data2.PassengerId.isin(Testlist)]\n",
    "Test = Data2.loc[Data2.PassengerId.isin(Testlist)]\n",
    "print('Dimension of trainset is',Train.shape)#891 x 14\n",
    "print('Dimension of testset is: ',Test.shape)#418 x 14\n",
    "Train.dtypes.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_y = Train.Survived\n",
    "Train_X = Train[[ 'Age', 'Fare', 'NumFamily', 'Sex_male',\n",
    "                 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Others', 'Embarked_Q',\n",
    "                 'Embarked_S', 'Pclass_2', 'Pclass_3']]\n",
    "#Train_X = Train[['PassengerId', 'Age', 'Fare', 'NumFamily', 'Sex_male', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Others','Embarked_Q', 'Embarked_S', 'Pclass_2', 'Pclass_3']]\n",
    "Test_y = Test.Survived\n",
    "Test_X = Test[['Age', 'Fare', 'NumFamily', 'Sex_male',\n",
    "                 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Others', 'Embarked_Q',\n",
    "                 'Embarked_S', 'Pclass_2', 'Pclass_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x106cb6a20>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(Train_X, Train_y)\n",
    "\n",
    "Y_prediction = random_forest.predict(Test_X)\n",
    "\n",
    "importances = pd.DataFrame({'feature':Train_X.columns,'importance':np.round(random_forest.feature_importances_,3)})\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
    "importances.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Introduction of Evaluation Matrics\n",
    "Before using multiple ML algorithms, let's talk about Evaluation Matrics in advance. How to define how effective an algorithm is? If it is hard to understand, maybe we can think about a simple question: How to tell if a restaurant is a good one. Are we judging the restaurants on the basis of their hotpot? Sushi? Ramen? Service? Or even In-store decoration?\n",
    "\n",
    "This is what metrics do. As we know, Machine Learning algorithms work on constructive feedback principle. If we apply ML algorithms, get feedback from the metrics, make improvement(try other tuning parameters) and continue until you achieve a desirable criterion. We use evaluation matrics to explain the performance of an algorithm. Different performance metrics are used to evaluate different machine learning algorithms. In House Sale in King County project, as it is a regression problem, we use RMSE as metric. Titanic Survival project is a binary classification problem, so we will implement some other type of metrics, such as **Confusion Matrix**, **Accuracy**, **Specificity**, **Sensitivity**,  **Precision-Recall**, **F1-score**, **ROC-AUC**, etc.\n",
    "\n",
    "<img src = \"confusionmatrix.png\" width = 550>\n",
    "\n",
    "In classification problem, after prediction, we can compare the predicted labels with the true labels to see whether our prediction is accurate or not. **Confusion Matrix** is one of the most intuitive and easiest metrics used for finding the correctness and accuracy of the model. The Confusion matrix itself is not a performance measure, but almost all of the performance metrics are based on Confusion Matrix and the numbers inside. \n",
    "\n",
    "**True Positives** are the cases when the actual class of the data point was True and the predicted is also True. **True Nagetives** are the cases when the actual class of data was False and the predicted is also Flase. **False Positives** are the cases when the actual class of the data point was False and the predicted is True. **False Negatives** are the cases when the actual class of data was True and the predicted is False.\n",
    "\n",
    "Besides, there are some other matrics based on confusion matrix:\n",
    "\n",
    "* **False Positive Rate** (Type I Error): b/(b+d), \n",
    "* **False Negative Rate** (Type II Error): c/(a+c),\n",
    "* **Specificity** (True Negative RATE): Number of items correctly identified as negative out of total negatives\n",
    "**Specificity** = d/(b+d) = 1 - Recall\n",
    "* **Sensitivity** (True Positive Rate/Recall): Number of items correctly identified as positive out of total true positives\n",
    "**Sensitivity** = a/(a+c)\n",
    "* **Accuracy**: Percentage of total items classified correctly\n",
    "**Accuracy** = (a+d)/(a+b+c+d)\n",
    "* **Precision**:  Number of items correctly identified as positive out of total items identified as positive\n",
    "**Precision** = a/(a+b)\n",
    "* **Recall**: Number of items correctly identified as positive out of total true positives\n",
    "**Recall** = a/(a+c)\n",
    "* **F1-score**: A harmonic mean of precision and recall\n",
    "**F1** = 2*Precision*Recall/(Precision + Recall)\n",
    "* **ROC-AUC** ROC(receiver operating characteristic) curve a plot which illustrates the diagnostic ability of a classification algorithm at different classification thresholds. X axis is false positive(sensitivity) rate and y axis is true positive rate(1-specificity). ROC-AUC measures the entire two-dimensional area underneath the entire ROC curve  from (0,0) to (1,1). AUC larger, the algorithm predict better.\n",
    "\n",
    "<img src = \"ROCAUC.png\" width = 350>\n",
    "\n",
    "So many different metrics listed above. **When to use what. It may be a problem we need to talk about.**\n",
    "\n",
    "##### False Positive/False Negative:\n",
    "We know that there will be some error associated with every model that we used for predicting the true class of target variable. This will result in False Positive and False Nagetive. It depends on different needs of the problem to determine which one should be minimized.\n",
    "\n",
    "(1) Missing a cancer patient will be a huge mistake. If we want to correctly identify all cancerous patients from people, in order to catch all cancer cases, minimize FALSE NAGETIVE is necessary. \n",
    "\n",
    "(2) However, if we want to classify whether an email is spam or not, it seems worse to classify a important email to spam than classify a spam email as important. In this situation, minimize FALSI POSITIVE is nice.\n",
    "\n",
    "##### Accuracy\n",
    "Accuracy is a good measure when the target variable classes are nearly balanced. **But if target variable is not quite balanced, accuracy will not be a good metric.** For example, if we want to detect survival from 100 people and find only 5 are survived from Titanic disaster. Even though the model is terrible enough, all people are labeled as 'non-cancerous', the accuracy is still 95% high. In this situation, accuracy is not a good metric.\n",
    "\n",
    "##### ROC-AUC\n",
    "Similar as accuracy, ROC-AUC is also a good measure when the target variable classes are nearly balanced. **But if target variable is not quite balanced, it will not be a good metric either.** Here is a kaggle example: https://www.kaggle.com/lct14558/imbalanced-data-why-you-should-not-use-roc-curve. To solve this problem, we can use Precision-Recall AUC instead of ROC-AUC.\n",
    "\n",
    "##### Precision and Recall\n",
    "Precision is a measure which tells us what proportion of people we predict as survived, acurally survived. Precision gives us information about performance with respect to false positive. if we want to minimize False Positive, make precision close to 100%.\n",
    "\n",
    "Recall is a measure which returns what proportion of truly survived people was predicted by the algprithm as survived. Recall gives us information about a classifier's performance with respect to false negative. If we focus more on minimizing False Negative, we want recall close to 100%.\n",
    "\n",
    "The precision-recall plot is shown as follows:\n",
    "\n",
    "<img src = \"PRAUC.png\" width = 250>\n",
    "\n",
    "**Comparing with ROC curve, precision-recall curve seems not effected much by inbalanced target variable.** Please see a comparison example in https://www.kaggle.com/general/7517#post41179.\n",
    "\n",
    "<img src = \"PRAUCexample.png\" width = 650>\n",
    "\n",
    "##### F1-score\n",
    "We use a balanced mean which is called harmonic mean to combine precision and recall together. Harmonic mean is a kind of an average when x and y are equal. But if x and y are different, marmonic mean returns a smaller one, giving the model an appropriate score rather than just an arithmetric mean.\n",
    "\n",
    "<img src = \"harmonic.png\" width = 180>\n",
    "\n",
    "We usually use both PR-AUC and F1-score when the target variable is inbalanced.\n",
    "\n",
    "In our Titanic project, which metric should we use? Let's check the balance of column 'Survived' at first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    549\n",
      "1    342\n",
      "Name: Survived, dtype: int64\n",
      "Column Survived is balanced in train set\n"
     ]
    }
   ],
   "source": [
    "print(Train_y.value_counts())\n",
    "print('Column Survived is balanced in train set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    266\n",
      "1    152\n",
      "Name: Survived, dtype: int64\n",
      "Column Survived is balanced in test set\n"
     ]
    }
   ],
   "source": [
    "print(Test_y.value_counts())\n",
    "print('Column Survived is balanced in test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As 'Survived' is balanced in Titanic dataset, accuracy will be a good choice.\n",
    "\n",
    "### 3.6 Machine Learning Methods\n",
    "\n",
    "Since the problem we are trying to solve is a classification problem. We are going to use bunch of classification algorithms to get the best prediction. I will use the following models and describe them along the way to give a better perspective.\n",
    "<img src = \"Classification.png\" width = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1 Naive Bayes\n",
    "Naive Bayes algorithm is a simple but effective algorithm for classification based on Bayes' Theorem with an assumption of independence among predictor features. Given a new vector of features, our goal is to classify whether a passenger could be surveved or not, i.e. P(Survival|f1, ..., fn). We apply bayes law(equation 1) as follows:\n",
    "<img src = \"bayeslaw.png\" width = 350>\n",
    "As P(Survival) is easy to calculate, and P(f1, ..., fn) is not related to 'Survived', we only need to consider P(f1, ..., fn | Survival). Applying Maximum Likelihood Estimation(MLE) to solve the problem, we have to choose 'Survival' value(0/1) for which P(f1, ..., fn|Survival) is the highest. Please see the equation 2:\n",
    "\n",
    "<img src = \"bayeslaw0.png\" width = 280>\n",
    "Using conditional probability formula and the independent assumption, we have equation 3:\n",
    "<img src = \"bayeslaw2.png\" width = 400>\n",
    "\n",
    "Well, a problem is coming: How to model the probability function P(fi|Survived)? \n",
    "* If fi is continous feature, it follows Gaussian(Normal) Distribution. \n",
    "* If fi is discrete feature, it follows Nultinomial Distribution.\n",
    "* If fi is binary feature, it follows Bernoulli Distribution.\n",
    "\n",
    "Combining the Distribitons together, let's partially differentiate the above equation 3 with respect to the parameters such as mean and std. In order to find maximum likelihood estimators let’s equal above value to 0, then we will calculate the parameters. We do prediction and calculate the probability of Survival given a vector of features after that.\n",
    "\n",
    "In our Titanic project, we fit a naive bayes model in train dataset, do prediction, set a threshold to classify the probability into 0 or 1 and calculate evaluation metrics in test set. BEFORE these steps, the first thing is to double check the assumption of naive bayes: **Feature Independence**\n",
    "\n",
    "To check multicolinearity, we choose VIF(variance inflation factor) score and drop variables whose is more than 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Multicolinearity BY VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n",
    "\n",
    "def calculate_vif_(X, thresh=5.0):\n",
    "    variables = list(range(X.shape[1]))\n",
    "    dropped = True\n",
    "    while dropped:\n",
    "        dropped = False\n",
    "        vif = [variance_inflation_factor(X.iloc[:, variables].values, ix)\n",
    "               for ix in range(X.iloc[:, variables].shape[1])]\n",
    "        maxloc = vif.index(max(vif))\n",
    "        if max(vif) > thresh:\n",
    "            print('dropping \\'' + X.iloc[:, variables].columns[maxloc] +\n",
    "                  '\\' at index: ' + str(maxloc))\n",
    "            del variables[maxloc]\n",
    "            dropped = True\n",
    "    print('Remaining variables:')\n",
    "    print(X.columns[variables])\n",
    "    return X.iloc[:, variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping 'Sex_male' at index: 3\n",
      "dropping 'Title_Mr' at index: 4\n",
      "Remaining variables:\n",
      "Index(['Age', 'Fare', 'NumFamily', 'Title_Miss', 'Title_Mrs', 'Title_Others',\n",
      "       'Embarked_Q', 'Embarked_S', 'Pclass_2', 'Pclass_3'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>NumFamily</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Others</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.273456</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.473882</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.323563</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.436302</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.436302</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age      Fare  NumFamily  Title_Miss  Title_Mrs  Title_Others  \\\n",
       "0  0.273456  0.014151          1           0          0             0   \n",
       "1  0.473882  0.139136          1           0          1             0   \n",
       "2  0.323563  0.015469          0           1          0             0   \n",
       "3  0.436302  0.103644          1           0          1             0   \n",
       "4  0.436302  0.015713          0           0          0             0   \n",
       "\n",
       "   Embarked_Q  Embarked_S  Pclass_2  Pclass_3  \n",
       "0           0           1         0         1  \n",
       "1           0           0         0         0  \n",
       "2           0           1         0         1  \n",
       "3           0           1         0         0  \n",
       "4           0           1         0         1  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_X1 = calculate_vif_(Train_X)\n",
    "Train_X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>NumFamily</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Title_Others</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>0.430039</td>\n",
       "      <td>0.015282</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>0.586622</td>\n",
       "      <td>0.013663</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>0.774521</td>\n",
       "      <td>0.018909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>0.336089</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0.273456</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age      Fare  NumFamily  Title_Miss  Title_Mrs  Title_Others  \\\n",
       "891  0.430039  0.015282          0           0          0             0   \n",
       "892  0.586622  0.013663          1           0          1             0   \n",
       "893  0.774521  0.018909          0           0          0             0   \n",
       "894  0.336089  0.016908          0           0          0             0   \n",
       "895  0.273456  0.023984          2           0          1             0   \n",
       "\n",
       "     Embarked_Q  Embarked_S  Pclass_2  Pclass_3  \n",
       "891           1           0         0         1  \n",
       "892           0           1         0         1  \n",
       "893           1           0         1         0  \n",
       "894           0           1         0         1  \n",
       "895           0           1         0         1  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test_X1 = Test_X.drop(['Sex_male','Title_Mr'],axis = 1)\n",
    "Test_X1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[245  21]\n",
      " [  6 146]]\n",
      "-----------------------------------------------------------\n",
      "Precision:  0.87\n",
      "-----------------------------------------------------------\n",
      "Recall:  0.96\n",
      "-----------------------------------------------------------\n",
      "Accuracy:  93.5 %\n",
      "-----------------------------------------------------------\n",
      "F1 Score:  0.92\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(Train_X1, Train_y)\n",
    "y_pred = gaussian.predict(Test_X1)\n",
    "gaussianNB_accy = round(accuracy_score(y_pred, Test_y), 3)\n",
    "print('Confusion Matrix: ')\n",
    "print( confusion_matrix(Test_y, y_pred))\n",
    "print('-----------------------------------------------------------')\n",
    "print(\"Precision: \", round(precision_score(Test_y, y_pred),2))\n",
    "print('-----------------------------------------------------------')\n",
    "print(\"Recall: \",round(recall_score(Test_y, y_pred),2))\n",
    "print('-----------------------------------------------------------')\n",
    "print(\"Accuracy: \",round(gaussianNB_accy * 100,2),'%')\n",
    "print('-----------------------------------------------------------')\n",
    "print('F1 Score: ',round(f1_score(Test_y, y_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Naive Bayes classification performs well with an accuracy of 0.935. \n",
    "\n",
    "Usually, Naive Bayes is an easy, fast and simple to implement classification method and works well for small data, Naive Bayes also performs well even if the Naive Assumption is not perfectly met(Assumption is violated, NB is still optimal). This the reason why it is always be used as a baseline during model comparison. \n",
    "\n",
    "However, when we use Naive Bayes, we usually need to remove correlated features in advance to avoid multicolinearity. If a categorical variable has a category in test data set which was not observed in training data set, then the model will assign a zero probability. It will not be able to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation. Sklearn applies Laplace smoothing by default when you train a Naive Bayes classifier.\n",
    "\n",
    "Speed: The main cause for the fast speed of naive Bayes training is that it converges toward its asymptotic accuracy at a different rate than other methods, like logistic regression, support vector machines, and so on. Naive Bayes parameter estimates converge toward their asymptotic values in order of log(n) examples, where n is number of dimensions. In contrast, logistic regression parameter estimates converge more slowly, requiring order n examples. It is also observed that in several datasets logistic regression outperforms naive Bayes when many training examples are available in abundance, but naive Bayes outperforms logistic regression when training data is scarce.\n",
    "\n",
    "Citation: (1) https://blog.sicara.com/naive-bayes-classifier-sklearn-python-example-tips-42d100429e44 \n",
    "\n",
    "(2) https://towardsdatascience.com/bayes-classifier-with-maximum-likelihood-estimation-4b754b641488"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1 Logistic Regression with Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2 Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
